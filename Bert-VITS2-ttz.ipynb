{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9sOy6u9eE6-J"},"outputs":[],"source":["!git clone https://github.com/YYuX-1145/Bert-VITS2-Integration-package"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qOVPRHnAL5yf"},"outputs":[],"source":["%cd /content/Bert-VITS2-Integration-package\n","!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"51ikznVcOPgZ"},"outputs":[],"source":["#通过直链下载必要的模型文件\n","%cd /content/Bert-VITS2-Integration-package/emotional/wav2vec2-large-robust-12-ft-emotion-msp-dim\n","!wget https://huggingface.co/audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim/resolve/main/pytorch_model.bin"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QfEXCEGtFMMv"},"outputs":[],"source":["%cd /content/Bert-VITS2-Integration-package\n","!mkdir -p Data/ttz\n","%cd /content/Bert-VITS2-Integration-package/data/ttz\n","!wget https://github.com/baka-9/My-Bert-VITS2-File/releases/download/Bert-VITS2-Mods/config.json\n","!wget https://github.com/baka-9/My-Bert-VITS2-File/releases/download/Bert-VITS2-Mods/G_7000.pth"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pDOAWD58hMHJ"},"outputs":[],"source":["%cd /content/Bert-VITS2-Integration-package\n","!wget https://github.com/baka-9/My-Bert-VITS2-File/releases/download/colab/config.yml"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"aDAmKb_rFVR-"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/Bert-VITS2-Integration-package\n","Ignored unknown kwarg option normalize\n","Ignored unknown kwarg option normalize\n","Ignored unknown kwarg option normalize\n","Ignored unknown kwarg option normalize\n","Some weights of EmotionModel were not initialized from the model checkpoint at ./emotional/wav2vec2-large-robust-12-ft-emotion-msp-dim and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","| numexpr.utils | INFO | NumExpr defaulting to 2 threads.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n","  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n","| utils | INFO | Loaded checkpoint '/content/Bert-VITS2-Integration-package/Data/ttz/G_7000.pth' (iteration 242)\n","/usr/local/lib/python3.10/dist-packages/gradio/components/dropdown.py:163: UserWarning: The value passed into gr.Dropdown() is not in the list of choices. Please update the list of choices to include: null or set allow_custom_value=True.\n","  warnings.warn(\n","推理页面已开启!\n","Running on local URL:  http://127.0.0.1:7860\n","Running on public URL: https://a943ed5013a072b470.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n","Building prefix dict from the default dictionary ...\n","| jieba | DEBUG | Building prefix dict from the default dictionary ...\n","Dumping model to file cache /tmp/jieba.cache\n","| jieba | DEBUG | Dumping model to file cache /tmp/jieba.cache\n","Loading model cost 0.689 seconds.\n","| jieba | DEBUG | Loading model cost 0.689 seconds.\n","Prefix dict has been built successfully.\n","| jieba | DEBUG | Prefix dict has been built successfully.\n","Some weights of the model checkpoint at ./bert/chinese-roberta-wwm-ext-large were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","/usr/local/lib/python3.10/dist-packages/gradio/processing_utils.py:354: UserWarning: Trying to convert audio automatically from float32 to 16-bit int format.\n","  warnings.warn(warning.format(data.dtype))\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n","    output = await route_utils.call_process_api(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n","    output = await app.get_blocks().process_api(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1522, in process_api\n","    result = await self.call_function(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1144, in call_function\n","    prediction = await anyio.to_thread.run_sync(\n","  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n","    return await get_asynclib().run_sync_in_worker_thread(\n","  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n","    return await future\n","  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n","    result = context.run(func, *args)\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 674, in wrapper\n","    response = f(*args, **kwargs)\n","  File \"/content/Bert-VITS2-Integration-package/webui.py\", line 339, in tts_fn\n","    generate_audio(\n","  File \"/content/Bert-VITS2-Integration-package/webui.py\", line 56, in generate_audio\n","    audio = infer(\n","  File \"/content/Bert-VITS2-Integration-package/infer.py\", line 227, in infer\n","    net_g.infer(\n","  File \"/content/Bert-VITS2-Integration-package/models.py\", line 1042, in infer\n","    z = self.flow(z_p, y_mask, g=g, reverse=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/Bert-VITS2-Integration-package/models.py\", line 156, in forward\n","    x = flow(x, x_mask, g=g, reverse=reverse)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/Bert-VITS2-Integration-package/modules.py\", line 564, in forward\n","    h = self.enc(h, x_mask, g=g)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/Bert-VITS2-Integration-package/attentions.py\", line 104, in forward\n","    attn_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 490.91 GiB. GPU 0 has a total capacty of 14.75 GiB of which 9.26 GiB is free. Process 40406 has 5.48 GiB memory in use. Of the allocated memory 4.16 GiB is allocated by PyTorch, and 263.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n","    output = await route_utils.call_process_api(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n","    output = await app.get_blocks().process_api(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1522, in process_api\n","    result = await self.call_function(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1144, in call_function\n","    prediction = await anyio.to_thread.run_sync(\n","  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n","    return await get_asynclib().run_sync_in_worker_thread(\n","  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n","    return await future\n","  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n","    result = context.run(func, *args)\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 674, in wrapper\n","    response = f(*args, **kwargs)\n","  File \"/content/Bert-VITS2-Integration-package/webui.py\", line 339, in tts_fn\n","    generate_audio(\n","  File \"/content/Bert-VITS2-Integration-package/webui.py\", line 56, in generate_audio\n","    audio = infer(\n","  File \"/content/Bert-VITS2-Integration-package/infer.py\", line 227, in infer\n","    net_g.infer(\n","  File \"/content/Bert-VITS2-Integration-package/models.py\", line 1042, in infer\n","    z = self.flow(z_p, y_mask, g=g, reverse=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/Bert-VITS2-Integration-package/models.py\", line 156, in forward\n","    x = flow(x, x_mask, g=g, reverse=reverse)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/Bert-VITS2-Integration-package/modules.py\", line 564, in forward\n","    h = self.enc(h, x_mask, g=g)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/Bert-VITS2-Integration-package/attentions.py\", line 104, in forward\n","    attn_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 490.91 GiB. GPU 0 has a total capacty of 14.75 GiB of which 9.26 GiB is free. Process 40406 has 5.48 GiB memory in use. Of the allocated memory 4.16 GiB is allocated by PyTorch, and 263.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\n","The above exception was the direct cause of the following exception:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 501, in process_events\n","    response = await self.call_prediction(awake_events, batch)\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 465, in call_prediction\n","    raise Exception(str(error) if show_error else None) from error\n","Exception: None\n","/content/Bert-VITS2-Integration-package/webui.py:539: FutureWarning: Pass sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n","  lambda x: librosa.load(x, 16000)[::-1],\n","/usr/local/lib/python3.10/dist-packages/gradio/processing_utils.py:354: UserWarning: Trying to convert audio automatically from float32 to 16-bit int format.\n","  warnings.warn(warning.format(data.dtype))\n","/content/Bert-VITS2-Integration-package/get_emo.py:17: FutureWarning: Pass sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n","  wav, sr = librosa.load(path, 16000)\n","2023-12-03 10:57:13.253827: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-03 10:57:13.253887: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-03 10:57:13.253928: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-03 10:57:14.942407: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/content/Bert-VITS2-Integration-package/webui.py:539: FutureWarning: Pass sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n","  lambda x: librosa.load(x, 16000)[::-1],\n","/usr/local/lib/python3.10/dist-packages/gradio/processing_utils.py:354: UserWarning: Trying to convert audio automatically from float32 to 16-bit int format.\n","  warnings.warn(warning.format(data.dtype))\n"]}],"source":["%cd /content/Bert-VITS2-Integration-package\n","!python webui.py"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPnYTV17SLmdckn6ofaKazu","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}